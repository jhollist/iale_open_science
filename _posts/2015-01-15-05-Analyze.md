---
title: "05 - Analyze: Base"
author: Jeffrey W. Hollister
layout: post_page
---


The focus of this workshop hasn't really been statistics, its been more about R, the language.  But, it is pretty much impossible to talk a lot about R without getting into stats, as that is what draws most people to R in the first place.  So, we will spend a little bit of time on it.  In this lesson we will touch on some very simple stats that we can do with base R as well as provide a quick (i.e. top of my head as I write this) list of what can be done in base R.  

##Quick Links to Exercises and R code
- [Lesson 5 R Code](/introR/rmd_posts/2015-01-15-05-Analyze.R): All the code from this post in an R Script.
- [Exercise 1](#exercise-1): Run some basic statistical test and build a simple model with the NLA data

##Lesson Goals
- Conduct basic statistical analyses with base R
- Get a taste of wide array of analyses in base R

##Base statistics
The capabilities that come out of the box with R are actually quite good and used to (i.e. before R) cost you quite a bit to access.  Now it all comes for free!  Some of the things you can do with R without any additional packages inlcude: logistic regression (and all manner of generalized linear models), correlation, principle components analysis, chi-squared tests, clustering, loess, ANOVA, MANOVA, ...  In short, we can do a lot without moving out of base r and `stats`.

We will talk about a few analyses just to show the tip of the iceberg of what is available.

### t-tests
A t-test is done simply with `t.test()` and you control the specifics (paired, two-sided, etc.) with options.  For the simple case of comparing the difference of two means we can use all of the defaults:


{% highlight r %}
x<-rnorm(30,mean=3,sd=2)
y<-rnorm(30,mean=10,sd=5)
xy_tt<-t.test(x,y)
xy_tt
{% endhighlight %}



{% highlight text %}
## 
## 	Welch Two Sample t-test
## 
## data:  x and y
## t = -5.7302, df = 36.157, p-value = 1.572e-06
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -8.103956 -3.867523
## sample estimates:
## mean of x mean of y 
##  2.962761  8.948501
{% endhighlight %}

`t.test()` can also take a formula specification as input. I mentioned this briefly in the previous lesson.  At it's simplest a formula takes the form of `y ~ x`.  The tilde is used in place of the equals sign.  For a t-test that is all we need to know.  


{% highlight r %}
#Lets pick another dataset, ToothGrowth.
#Looking at diff in tooth length by groups specified in supp
rbind(head(ToothGrowth),tail(ToothGrowth))
{% endhighlight %}



{% highlight text %}
##     len supp dose
## 1   4.2   VC  0.5
## 2  11.5   VC  0.5
## 3   7.3   VC  0.5
## 4   5.8   VC  0.5
## 5   6.4   VC  0.5
## 6  10.0   VC  0.5
## 55 24.8   OJ  2.0
## 56 30.9   OJ  2.0
## 57 26.4   OJ  2.0
## 58 27.3   OJ  2.0
## 59 29.4   OJ  2.0
## 60 23.0   OJ  2.0
{% endhighlight %}



{% highlight r %}
t.test(ToothGrowth$len~ToothGrowth$supp)
{% endhighlight %}



{% highlight text %}
## 
## 	Welch Two Sample t-test
## 
## data:  ToothGrowth$len by ToothGrowth$supp
## t = 1.9153, df = 55.309, p-value = 0.06063
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.1710156  7.5710156
## sample estimates:
## mean in group OJ mean in group VC 
##         20.66333         16.96333
{% endhighlight %}
There's a lot more you can do with `t.test()`, but you'll have to rely on `?t.test` for more info.

### Correlation
Next let's take a look at correlations.  Little inspiration is striking, so `iris` it is.


{% highlight r %}
#A simple correlation
cor(iris$Petal.Width,iris$Petal.Length)
{% endhighlight %}



{% highlight text %}
## [1] 0.9628654
{% endhighlight %}



{% highlight r %}
#And a test of that correlation
cor.test(iris$Petal.Width,iris$Petal.Length)
{% endhighlight %}



{% highlight text %}
## 
## 	Pearson's product-moment correlation
## 
## data:  iris$Petal.Width and iris$Petal.Length
## t = 43.3872, df = 148, p-value < 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.9490525 0.9729853
## sample estimates:
##       cor 
## 0.9628654
{% endhighlight %}



{% highlight r %}
#A data frame as input returns a correlation matrix
cor(iris)
{% endhighlight %}



{% highlight text %}
## Error in cor(iris): 'x' must be numeric
{% endhighlight %}



{% highlight r %}
#Oops, non-numeric data.  Lets use dplyr to get what we want and pipe to cor
library(dplyr)
select(iris,-Species) %>% 
  cor()
{% endhighlight %}



{% highlight text %}
##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411
## Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259
## Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654
## Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000
{% endhighlight %}

If you look at the help for `cor()`, you'll see two main optional arguments.  First is the `use` argument which allows you to use the entire dataset or select complete cases which is useful when you have `NA` values.  There are several options.  Also, the default correlation method is for Pearson's.  If you would like to us non-parametric correlations (e.g. rank), you specify that here.

### Linear Regression
Next let's take a look at linear regression.  One of the common ways of accessing linear regressions is with `lm()`.  We have already seen the formula object so there isn't too much that is new here.  Some of the options though are new and useful.  Let's take a look:


{% highlight r %}
lm(Ozone~Temp,data=airquality)
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = Ozone ~ Temp, data = airquality)
## 
## Coefficients:
## (Intercept)         Temp  
##    -146.995        2.429
{% endhighlight %}



{% highlight r %}
#Not much info, so save to object and use summary
lm_aq1<-lm(Ozone~Temp,data=airquality)
summary(lm_aq1)
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = Ozone ~ Temp, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.729 -17.409  -0.587  11.306 118.271 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***
## Temp           2.4287     0.2331  10.418  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 23.71 on 114 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.4877,	Adjusted R-squared:  0.4832 
## F-statistic: 108.5 on 1 and 114 DF,  p-value: < 2.2e-16
{% endhighlight %}



{% highlight r %}
#And now a multiple linear regression
lm_aq2<-lm(Ozone~Temp+Wind+Solar.R,data=airquality)
summary(lm_aq2)
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = Ozone ~ Temp + Wind + Solar.R, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.485 -14.219  -3.551  10.097  95.619 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -64.34208   23.05472  -2.791  0.00623 ** 
## Temp          1.65209    0.25353   6.516 2.42e-09 ***
## Wind         -3.33359    0.65441  -5.094 1.52e-06 ***
## Solar.R       0.05982    0.02319   2.580  0.01124 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 21.18 on 107 degrees of freedom
##   (42 observations deleted due to missingness)
## Multiple R-squared:  0.6059,	Adjusted R-squared:  0.5948 
## F-statistic: 54.83 on 3 and 107 DF,  p-value: < 2.2e-16
{% endhighlight %}

All of your standard modelling approaches (and then some) are available in R including typical variable selection techniques (e.g. stepwise with AIC) and logistic regression, which is implemented with the rest of the generalized linear models in `glm()`.  Interaction terms can be specified directly in the model, but there are several ways to do so (x*y, x:y, x^y).  Lastly, if you are interested in more involved or newer approaches these will be implemented (most likely) with additional packages and will not be available in base R or `stats`.



##Excercise 1
For this exercise, let's start to look at some of the statistical tests and relationships in the NLA data.

1. First, lets take a look at the man made and natural lakes.  Add a section to your script that tests for a difference in the mean value of TURB, NTL, PTL, CHLA, and SECMEAN between man made and natural lakes.  For this we will have the script save each test to a new object and print the results to the screen.

2. Next, lets build some linear models that build a linear model to be used to predict CHLA.  Again save your model to an object and print the summary of that model to the screen
