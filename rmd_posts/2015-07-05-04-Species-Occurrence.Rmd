---
title: "04 - Using Open Species Occurrence Data in R"
author: Scott A. Chamberlain
layout: post_page
---

```{r setup, echo=FALSE, warning=FALSE, purl=FALSE, message=FALSE}
options(repos = "http://cran.rstudio.com/")
knitr::opts_chunk$set(
  fig.path = "../figure/"
)
if (!require("rgbif")) {
 install.packages("rgbif")
}
if (!require("spocc")) {
 install.packages("spocc")
}
if (!require("spoccutils")) {
 install.packages("spoccutils")
}
if (!require("taxize")) {
 install.packages("taxize")
}
if (!require("rgdal")) {
 install.packages("rgdal")
}
if (!require("geojsonio")) {
 install.packages("geojsonio")
}

library("rgbif")
library("spocc")
library("spoccutils")
library("taxize")
library("geojsonio")
```

Species occurrence data is available in increasingly large quantities, as open data, available on the web.
Accessing the data is not always straight-forward. Potential problems arise due to dependence on an internet
connection, data size (especially over slow internet connections), dirty data, species name conflicts, and more.

This lesson will go through some ways to collect open species occurrence data in R, and wrap up with
demos of how to get species occurrence data back into familiar data formats (e.g., Spatial R objects,
geojson, etc.).

## Lesson Goals

  - Intro to Open Data via APIs & other web services
  - Using `rgbif` and `spocc` to access species occurence data
  - Using `taxize` to access and parse taxonomic information
  - Coverting `spocc` output to `sp`

## Quick Links to Exercises and R code

  - [Exercise 1](#exercise-1): Search for species occurrence data, make a simple map
  - [Exercise 2](#exercise-2): Clean taxonomic names
  - [Exercise 3](#exercise-3): Convert occurrence data to other formats


## Open data on the web

* GUI interfaces (no scraping allowed/possible)
* HTML (scraping)
* CSV/TXT/TSV etc.
* FTP
* APIs (SOAP, RESTful)

GUI interfaces are great!  That's where (nearly) everyone starts in their data search process. However, when you need to do more than a few queries/search tasks/etc., GUIs become very cumbersome.

Some websites allow you to scrape their content. That is, every web page has html code that defines that page. This is a structured language that you can collect in a programming lanugage like R, then parse out the exact things that you want. This is quite fragile though - that is, websites can update often, breaking any code you've written to scrape the site. In addition, html isn't the best way to move data around.

> Tools for scraping: rvest, xml2

Sometimes websites will provide data dumps in csv/text format. This is much better than scraping html, as it's easier for humans to understand, and we we can easily read in and manipulate the data. Always look for this type of data download as a better alternative to html scraping.

> Tools for importing csv: base R functions read.table/etc., & readr, data.table::fread, readxl

Data providers sometimes provide data via FTP (File Transfer Protocol). This is simply a file system that you can access data from. This is easy to use without a programming language - you can just go tot the FTP site and download data. But, as above, when dealing with more than a few files, it's best to write some code to automate download, data manipulation, etc.

> Tools for working with ftp: RCurl, curl, httr

The best situation, in most cases, is that a data provider provides a RESTful API (Application Programming Interface). Think of APIs as a set of instructions for one computer to talk to another, or one script to talk to another. For example, when you log into a site using your Facebook credentials - that's using one of the Facebook APIs.

APIs lay out a series of routes that define what data is avaiable, parameters to use to construct queries, and so on. On top of APIs, any programming language can build a client to interact with the API. This is the bees knees.

Caveat: When you have very large data, APIs sometimes are not best. See FTP.

> Tools for working with APIs: RCurl, curl, httr, httsnap

## Installation

Some packages you'll need install directly from CRAN (you may need special instructions for `rgdal`)

```{r eval=FALSE}
install.packages("rgdal")
install.packages(c("rgbif","taxize","geojsonio"))
```

For others we'll need `devtools`

```{r eval=FALSE}
devtools::install_github("ropensci/spocc")
```

```{r eval=FALSE}
install.packages(c("ggmap","grid","sp","rworldmap","RColorBrewer","httr","leafletR","gistr","maptools"))
devtools::install_github("ropensci/spoccutils")
```

Or install from binary

Download:

* Mac - https://github.com/ropensci/spoccutils/releases/download/v0.1.0/spoccutils_0.1.0.tgz
* Windows - https://github.com/ropensci/spoccutils/releases/download/v0.1.0/spoccutils_0.1.0.zip

Then install

```{r eval=FALSE}
install.packages("spoccutils_0.1.0.tgz", repos = NULL)
# OR
install.packages("spoccutils_0.1.0.zip", repos = NULL)
```

## Occurrence data

GBIF has probably the biggest collection of species occurrence data available on the web, and so we'll cover the `rgbif` R client first.

### rgbif

```{r}
library("rgbif")
```


#### Intro workflow

##### Search for names

A number of ways to search for names:

* `name_backbone()`
* `name_suggest()`
* `name_lookup()`
* `name_usage()`

```{r}
name_suggest(q = "Helianthus", limit = 10)
```

I often use `name_backbone()` to get to IDs that GBIF uses

```{r}
out <- name_backbone(name = "Oncorhynchus mykiss")
key <- out$usageKey
out[c('scientificName', 'usageKey')]
```

After we've found keys for the species we want, we may want to search for specific datasets (or skip to searching for occurrences).

##### Search for datasets

There are many functions for searching datasets, including

* `dataset_suggest()`
* `dataset_search()`
* `dataset_metrics()`
* `datasets()`
* `installations()`
* `networks()`
* `nodes()`
* `organizations()`

Let's try `dataset_search()`

```{r}
res <- dataset_search(query = "Oregon State University")
dkey <- 'b8ed9b7c-5b73-4e6d-a6e1-b0911a09f947'
res$data
```

We can see OSU in there, with datasetKey's for various collections, which we can use to narrow our search down (you don't have to do this of course). We'll use the Oregon State Ichthyology Collection key.

##### Search for records

* `occ_search()`
* `occ_get()`

There are others, but we'll focus on `occ_seach()`

```{r}
dat <- occ_search(taxonKey = key, datasetKey = dkey)
```

The output object is a data.frame, but has a special print method to display (hopefully) helpful information on what was returned, and a brief data.frame output so that you can quickly inspect data.


### spocc

```{r}
library("spocc")
```

#### Get data

There are many other sources of species occurrence data of course. Working with many sources has overhead in terms of learning new interfaces to each dataset, caveats, data types, taxonomies, etc. Thus, we've been working on a client for working with many sources of species occurrence data - with a single interface to all of them. It's called `spocc`.

`spocc` unifies access to biodiversity data across sources. First, we'll get data from just one resource: `GBIF`.

```{r}
out <- occ(query = 'Accipiter striatus', from = 'gbif', limit = 50)
out$gbif # GBIF data w/ metadata
out$ebird$data # empty
out$gbif$meta #  metadata, your query parameters, time the call executed, etc.
```

And you can search across many data sources easily by passing many values to the `from` parameter.

```{r}
out <- occ(query = 'Accipiter striatus', from = c('gbif', 'ebird'), limit = 50)
```

And easily squash together data

```{r}
df <- occ2df(out)
head(df); tail(df)
```

There's a certain set of global parameters that work for all data resources, but other settings you can still pass separately to each resource. For example:

```{r}
gopts <- list(country = 'US')
eopts <- list(county = "Alameda county")
(dat <- occ(query = 'Accipiter striatus', from = c('gbif', 'ecoengine'),
    gbifopts = gopts, ecoengineopts = eopts, limit = 100))
```

#### Make a map

Static map using ggplot2

```{r}
library("spoccutils")
map_ggplot(dat)
```

Static map using base R maps

```{r}
map_plot(dat)
```

Interactive map using leaflet

```{r eval = FALSE}
map_leaflet(dat)
```

![leaflet]({{ site.url }}/figure/../figure/leaflet_map.png)

## Exercise 1

Make a map!

1. Pick your favorite single species (or more than one), or favorite taxon group.
2. Search for occurrence data using `spocc` from at least __2__ data sources.
3. Make a map of the occurrence data.




## taxize

When dealing with spatial data, we're often dealing with biological specimens. Whenver that's the case, taxonomic names are a huge area of potential frustration.

`taxize` aims to solve your problems by giving you access to as many taxonomic name datasets as possible, all in one R package.

```{r}
library("taxize")
```


### Identifiers

Here, get Catalogue of Life identifier for the genus *Puma*

```{r}
(id <- get_colid("Puma", rows = 1))
```

Then we can pass those ids to other functions that act on those ids without any other input to do other cool things, like:

### Hierarchy of names

```{r}
classification(id)
```

### Downstream names

```{r}
downstream(id, downto = "species")
```

### Upstream names

```{r}
upstream(id, upto = "Family")
```

### Name synonyms

```{r}
synonyms("Poa annua", db = "tropicos", rows = 1)
```

### And lots more

We can do alot more with `taxize` - let's dig into an excercise to epxlore the package.

## Exercise 2

Take dirty names, clean them, then get additional taxonomic information, and output a table.

1. Get some taxonomic names - (hint: `names_list()`).
2. Get taxonomic identifiers for the names from a single data source (e.g., NCBI).
3. With the identifiers, get additional data on each taxon.



## Convert to spatial data formats

### sp & friends

`rgdal` has `writeOGR` and `readOGR`. These are your friends for reading and writing spatial data.

#### Convert data.frame to spatial classes

`data.frame` to `SpatialPointsDataFrame` class

```{r}
library("spocc")
library("sp")
out <- occ(query = 'Accipiter striatus', from = 'gbif', limit = 50)
df <- occ2df(out)
df <- na.omit(df)
df2 <- df
coordinates(df2) <- ~latitude + longitude
df$latitude <- NULL
df$longitude <- NULL
spdf <- SpatialPointsDataFrame(df2, data = df)
head(spdf)
```

Then you can go from there to lots of other things. For example, use the `SpatialPointsDataFrame` created above to save to a shape file.

```{r}
library("rgdal")
file <- tempfile()
dir.create("esrishape", showWarnings = FALSE)
writeOGR(spdf, "esrishape/out.shp", "", "ESRI Shapefile")
```

### GeoJSON/TopoJSON

`geojsonio` helps you convert R objects to GeoJSON and GeoJSON to nearly any of the `sp` spatial classes by parsing JSON then serializing into spatial classes, and back to GeoJSON (done right now via `rgdal`).

For example, convert a `data.frame` to GeoJSON...

...as an R list

```{r}
library("geojsonio")
(res <- geojson_list(us_cities[1:2,], lat = 'lat', lon = 'long'))
```

...as JSON

```{r}
as.json(res)
```

Or directly to JSON

```{r}
geojson_json(us_cities[1:2,], lat = 'lat', lon = 'long')
```

GeoJSON in JSON format is very widely used on the web, so is useful to be familiar with.

## Exercise 3

Part A)

1. Get occurrence data via `rgbif` or `spocc`
2. Creat a single `data.frame` from all occurrence data
3. Create a shapefile with that data.

Part B)

1. Get GeoJSON from somewhere on the web.
2. Read it into R.
3. Convert it to a spatial class, and make a simple plot of the data

