---
title: "02 - Tidy Data and Data Manipulation with R"
author: Betty J. Kreakie
layout: post_page
---

While I give brief introduction to the principles of tidy data, please install `tidyr` and `dplyr`.  If you run into problems with this, use the stickie pads and Scott, Jeff, or Bryan will be by to help.

**Principles of Tidy Data**

Nearly everything that I will cover in the section is stolen directly from some Hadley Wickham products.  In fact, if you just read the following paper, you should learn everything you need to know about data management (unless you are doing crazy database stuff- then you might need some intense instruction).


[Wickham, H. 2014. Tidy Data. Journal of Statistical Software 59.](http://vita.had.co.nz/papers/tidy-data.pdf)


> “Happy families are all alike; every unhappy family is unhappy in its own way”
> *Leo Tolstoy*

> “[T]idy datasets are all alike but every messy dataset is messy in its own way.”
> *Hadley Wickham*


It is estimated that 80% of data analysis time is spent cleaning and preparing data (Dasu T, Johnson T (2003). Exploratory Data Mining and Data Cleaning. John Wiley & Sons).  There will always be a need to devote some time to preparing data for specific analysis.  However, the ultimate goal is to reduce the time needed to deal with “messy data” issues.


Tidy data is a standard method of data storage for increased interpretability and integration.

*Why should you care about tidy data?*

  * Data analysis software is created based on a standardized data format
  * Data is super expensive and difficult to collect
  * Increase the ease and accuracy of data sharing
  * Because you are required to care

*The Principles of Tidy Data:*

1. Rows are observations
2. Variables are Columns
3. Observational Units are Tables


The rest of this section will be spent talking about (and playing with) two data manipulation packages: `tidyr` and `dplyr`.  To understand these packages (and other Wickham packages like `ggplot2`), it is first useful to make sure we have a basic understanding of pipes.  

If you need to execute several functions on an object, there are three ways to do this: use intermediate steps, nested functions, or pipes. With the intermediate steps, you essentially create a temporary data frame and use that as input to the next function. You can also nest functions (i.e. one function inside of another). This is handy, but can be difficult to read if too many functions are nested as the process from inside out. The last option, pipes, are a fairly recent addition to R.   Whenever you see %>%, read as “take… and then ….” 

Pipes allow us to:

1. Reduce the number of nested parenthesizes
2. Write code that reads left to right and more like actual pseudocode
3. Increase organization and more in-line with workflow


*Examples*

```{r, echo=TRUE, eval=TRUE, message=FALSE}
library('magrittr')
str1<- "Facts are stubborn"
str2<-"but statistics are more pliable"

##Example of intermediate step
play<-paste(str1,str2)
toupper(play)
 
##Example of nested functions 
toupper(paste(str1,str2))

##Pipes
str1 %>% substr(1,5)
str1 %>% paste(str2) %>% toupper()


```


**Package: `tidyr`**

The `tidyr` package is intended to deal with a lot of the common messy data issues.  If you were using `reshape` or `reshape2` to clean your data, `tidyr` should be easier and faster.  However, it is specifically designed for data cleaning and not reshaping or aggregate (i.e. it might not replace `reshape2` in your workflow).  There are two main functions with which we will work: gather() and separate().

gather() takes multiple columns and collapses into key-value pairs, duplicating all other columns as needed.

*Example*
```{r, echo=TRUE, eval=TRUE, message=FALSE}
library(tidyr)
messy<-data.frame(site=c("pawcatuck"," pettaquamscutt","pawtuxet"," pocasset","ponaganset"), a=c(56,76,43,25,21),b=c(123,234,187,198,23))

messy

messy %>%
  gather(treatment, count, a:b) %>% print()


```


Given either regular expression or a vector of character positions, separate() turns a single character column into multiple columns.  

*Example*
```{r, echo=TRUE, eval=TRUE, message=FALSE}
df <- data.frame(x = c("a.b", "a.d", "b.c"))%>% print()

df %>% separate(x, c("A", "B"))%>% print()

df <- data.frame(x = c("x: 123", "y: error: 7"))%>% print()
df %>% separate(x, c("key", "value"), ": ", extra = "merge") %>% print()


```

**Package: `dplyr`**

The package `dplyr` is a fairly new (2014) package that tries to provide easy tools for the most common data manipulation tasks. It is built to work directly with data frames. The thinking behind it was largely inspired by the package `plyr` which has been in use for some time but suffered from being slow in some cases.

`dplyr` is built around 5 verbs. These verbs make up the majority of the data manipulation you tend to do. You might need to:

1. select () certain columns of data. Can in use it place of subset() but with some fancy additions like contains(), starts_with() and, ends_with()
2. filter() your data to select specific rows
3. arrange() the rows of your data into an order
4. mutate() your data frame to contain new columns
5. summarise() chunks of you data in some way


Also has glimpse(), which you can use it like str().

```{r, echo=TRUE, eval=TRUE, message=FALSE}
library(dplyr)
glimpse(iris)

head(select(iris, contains("Petal"))) 

iris%>%filter(Species=="setosa")%>%head()

iris%>%arrange(Petal.Length)%>%head()

iris%>%arrange(desc(Petal.Length))%>%head()


iris%>%mutate(Sepal.Length.mm=Sepal.Length*10)%>%head()

iris%>%summarize(sepalMean=mean(Sepal.Length,na.rm=TRUE))


```

And just one more cool `dplyr` function, group_by()

```{r, echo=TRUE, eval=TRUE, message=FALSE}
group_by(iris,Species)%>%
  summarize(mean(Sepal.Length),
            mean(Sepal.Width),
            mean(Petal.Length),
            mean(Petal.Width))


```

As well as working with local in-memory data like data frames and data tables, `dplyr` also works with remote on-disk data stored in databases. Generally, if your data fits in memory there is no advantage to putting it in a database: it will only be slower and more hassle. The reason you’d want to use `dplyr` with a database is because either your data is already in a database (and you don’t want to work with static csv files that someone else has dumped out for you), or you have so much data that it does not fit in memory and you have to use a database. Currently `dplyr` supports the three most popular open source databases (SQLite, MySQL and PostgreSQL), and Google’s bigquery. [Link to `dplyr` database vignette](http://cran.r-project.org/web/packages/dplyr/vignettes/databases.html)


**Exercises**

```{r, echo=TRUE, eval=FALSE, message=FALSE}
###Use for #1-3
set.seed(10)
messy <- data.frame(
  id = 1:4,
  trt = sample(rep(c('control', 'treatment'), each = 2)),
  work.T1 = runif(4),
  home.T1 = runif(4),
  work.T2 = runif(4),
  home.T2 = runif(4)
)


```

1. Use gather() to turn columns work.T1, home.T1, work.T2and home.T2 into a key-value pair of key and time. 

2. Use separate() to split the key into location and time.  Try piping #1 and #2 together into one command.

3. Using `dplyr`, find the max time for the treatment and control.

4. Using the mtcars data set, what is the average mpg for all vehicles in the mtcars data set?  What is the average mpg for each number of cylinders? Create a new column for the log of the weight.

